{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2630,
     "status": "ok",
     "timestamp": 1740856788765,
     "user": {
      "displayName": "Francesco",
      "userId": "17757392889991151115"
     },
     "user_tz": -60
    },
    "id": "cFCI92wYdpJt",
    "outputId": "03726593-b513-4d1f-a49a-d60bcad195b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru==0.7.3 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: torch==2.5.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: typer==0.15.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: matplotlib==3.10.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: pyarrow==18.1.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (18.1.0)\n",
      "Requirement already satisfied: setuptools==75.1.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (75.1.0)\n",
      "Requirement already satisfied: protobuf==4.25.3 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (4.25.3)\n",
      "Requirement already satisfied: ultralytics==8.3.85 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (8.3.85)\n",
      "Requirement already satisfied: ray==2.43.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (2.43.0)\n",
      "Requirement already satisfied: albumentations==2.0.5 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (2.0.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from loguru==0.7.3) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from loguru==0.7.3) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from torch==2.5.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from torch==2.5.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from torch==2.5.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from torch==2.5.1) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from typer==0.15.1) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from typer==0.15.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from typer==0.15.1) (13.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from matplotlib==3.10.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from matplotlib==3.10.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from matplotlib==3.10.0) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from matplotlib==3.10.0) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from matplotlib==3.10.0) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from matplotlib==3.10.0) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from matplotlib==3.10.0) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from matplotlib==3.10.0) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from matplotlib==3.10.0) (2.9.0.post0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ultralytics==8.3.85) (4.11.0.86)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ultralytics==8.3.85) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ultralytics==8.3.85) (1.15.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ultralytics==8.3.85) (0.20.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ultralytics==8.3.85) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ultralytics==8.3.85) (9.0.0)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ultralytics==8.3.85) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ultralytics==8.3.85) (2.0.14)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ray==2.43.0) (4.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ray==2.43.0) (1.1.0)\n",
      "Requirement already satisfied: aiosignal in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ray==2.43.0) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from ray==2.43.0) (1.5.0)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from albumentations==2.0.5) (2.10.6)\n",
      "Requirement already satisfied: albucore==0.0.23 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from albumentations==2.0.5) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from albumentations==2.0.5) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from albucore==0.0.23->albumentations==2.0.5) (3.12.2)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from albucore==0.0.23->albumentations==2.0.5) (6.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations==2.0.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations==2.0.5) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.10.0) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.3.85) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.3.85) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.3.85) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.3.85) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from rich>=10.11.0->typer==0.15.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from rich>=10.11.0->typer==0.15.1) (2.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from jsonschema->ray==2.43.0) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from jsonschema->ray==2.43.0) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from jsonschema->ray==2.43.0) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from jsonschema->ray==2.43.0) (0.23.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\franc\\documents\\repository\\deeplearning-droneroadaccidentdetection\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer==0.15.1) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install loguru==0.7.3 python-dotenv==1.0.1 PyYAML==6.0.2 torch==2.5.1 tqdm==4.67.1 typer==0.15.1 matplotlib==3.10.0 pyarrow==18.1.0 setuptools==75.1.0 protobuf==4.25.3 ultralytics==8.3.85 ray==2.43.0 albumentations==2.0.5 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h04mIHIKdUZr"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO, settings\n",
    "import gc\n",
    "import json\n",
    "import locale\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = \"\"\"\n",
    "model: \"yolo12n\"\n",
    "wandb_project: \"EyeInTheSky_merged\"\n",
    "data: \"VisDrone.yaml\"\n",
    "train:\n",
    "  model: \"yolo12n\"\n",
    "  project: \"EyeInTheSky\"\n",
    "  data: \"VisDrone.yaml\"\n",
    "  task: detect\n",
    "  epochs: 1\n",
    "  batch: 16\n",
    "  workers: 8\n",
    "  seed: 42\n",
    "  plots: True\n",
    "  imgsz: 640\n",
    "  exist_ok: False\n",
    "  save: True\n",
    "  save_period: 10\n",
    "  val: True\n",
    "  warmup_epochs: 10\n",
    "  visualize: True\n",
    "  show: True\n",
    "  single_cls: False # (bool) train multi-class data as single-class\n",
    "  rect: False # (bool) rectangular training if mode='train' or rectangular validation if mode='val'\n",
    "  cos_lr: False\n",
    "  resume: False\n",
    "  amp: True # (bool) Automatic Mixed Precision (AMP) training, choices=[True, False], True runs AMP check\n",
    "  fraction: 1.0\n",
    "  freeze: None\n",
    "  cache: False\n",
    "val:\n",
    "  project: \"EyeInTheSky\"\n",
    "  name: \"YOLOv12-VisDrone-Validation\"\n",
    "  half: True\n",
    "  conf: 0.25\n",
    "  iou: 0.6\n",
    "  split: \"test\"\n",
    "  rect: True\n",
    "  plots: True\n",
    "  visualize: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> str:\n",
    "    try:\n",
    "        return 0 if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting device: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9U8n6O74dUZx"
   },
   "outputs": [],
   "source": [
    "# Load config\n",
    "\n",
    "# config = Config.load(\"../config/config.yaml\")\n",
    "config = yaml.safe_load(config_data)\n",
    "config.update({\"device\" : get_device()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wandb_key_colab() -> str:\n",
    "    try:\n",
    "        from google.colab import userdata # type: ignore\n",
    "\n",
    "        if userdata.get(\"WANDB_API_KEY\") is not None:\n",
    "            return userdata.get(\"WANDB_API_KEY\")\n",
    "        else:\n",
    "            raise ValueError(\"No WANDB key found\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_wandb_env(path: Path) -> str:\n",
    "    try:\n",
    "        from dotenv import dotenv_values # type: ignore\n",
    "\n",
    "        \"\"\"Get W&B API key from Colab userdata or environment variable\"\"\"\n",
    "\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Could not find .env file at {path}\")\n",
    "\n",
    "        print(f\"Loading secrets from {path}\")\n",
    "\n",
    "        secrets = dotenv_values(path)\n",
    "        print(f\"Found keys: {list(secrets.keys())}\")\n",
    "\n",
    "        if \"WANDB_API_KEY\" not in secrets:\n",
    "            raise KeyError(f\"WANDB_API_KEY not found in {path}. Available keys: {list(secrets.keys())}\")\n",
    "\n",
    "        return secrets['WANDB_API_KEY']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_wandb_key(path: Path = \"../.env\") -> str:\n",
    "    return get_wandb_key_colab() if get_wandb_key_colab() is not None else get_wandb_env(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ICqxkRBYK7T_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading secrets from ..\\.env\n",
      "Found keys: ['ROBOFLOW_API_KEY', 'WANDB_API_KEY']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\franc\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancescoperagine\u001b[0m (\u001b[33mfrancescoperagine-universit-degli-studi-di-bari-aldo-moro\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\franc\\Documents\\Repository\\DeepLearning-DroneRoadAccidentDetection\\notebooks\\wandb\\run-20250308_164224-ao3amsey</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged/runs/ao3amsey' target=\"_blank\">yolo12n_VisDrone.yaml_train</a></strong> to <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged' target=\"_blank\">https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged/runs/ao3amsey' target=\"_blank\">https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged/runs/ao3amsey</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_api_key = get_wandb_key()\n",
    "wandb.login(key=wandb_api_key, relogin=True)\n",
    "wandb.init(\n",
    "    project=config[\"wandb_project\"],\n",
    "    name=f\"{config['model']}_{config['data']}_train\",\n",
    ")\n",
    "settings.update({\"wandb\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data.dataset import YOLODataset\n",
    "from ultralytics.models.yolo.detect import DetectionTrainer\n",
    "from ultralytics.utils import colorstr, LOGGER\n",
    "import numpy as np\n",
    "\n",
    "class VisDroneDataset(YOLODataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for VisDrone that merges pedestrian (0) and people (1) classes.\n",
    "    Handles class remapping at the earliest possible stage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the merged names as a class attribute to be accessible from the trainer\n",
    "    merged_names = {\n",
    "        0: 'persona',\n",
    "        1: 'bicicletta',\n",
    "        2: 'auto',\n",
    "        3: 'furgone',\n",
    "        4: 'camion',\n",
    "        5: 'triciclo',\n",
    "        6: 'triciclo-tendato',\n",
    "        7: 'autobus',\n",
    "        8: 'motociclo'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Store original data before initialization if it exists in kwargs\n",
    "        self.original_data = kwargs.get('data', {}).copy() if 'data' in kwargs else None\n",
    "        \n",
    "        # Adjust data names before parent initialization to make verification pass\n",
    "        if self.original_data and 'names' in self.original_data:\n",
    "            # Create a temporary data object with 10 classes for verification\n",
    "            temp_data = self.original_data.copy()\n",
    "            # Ensure we have all 10 original class names for validation\n",
    "            if len(temp_data.get('names', {})) != 10:\n",
    "                temp_data['names'] = {\n",
    "                    0: 'pedestrian',\n",
    "                    1: 'people',\n",
    "                    2: 'bicycle',\n",
    "                    3: 'car',\n",
    "                    4: 'van',\n",
    "                    5: 'truck',\n",
    "                    6: 'tricycle',\n",
    "                    7: 'awning-tricycle',\n",
    "                    8: 'bus',\n",
    "                    9: 'motor'\n",
    "                }\n",
    "            # Replace data in kwargs\n",
    "            kwargs['data'] = temp_data\n",
    "        \n",
    "        # Initialize parent class with modified kwargs\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Log class mapping\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Using merged classes: {self.merged_names}\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"\n",
    "        Load and process labels with class remapping.\n",
    "        \"\"\"\n",
    "        # Get labels from parent method\n",
    "        labels = super().get_labels()\n",
    "        \n",
    "        # Process statistics\n",
    "        people_count = 0\n",
    "        shifted_count = 0\n",
    "        \n",
    "        # Process labels to merge classes\n",
    "        for i in range(len(labels)):\n",
    "            cls = labels[i]['cls']\n",
    "            \n",
    "            if len(cls) > 0:\n",
    "                # Count 'people' instances\n",
    "                people_mask = cls == 1\n",
    "                people_count += np.sum(people_mask)\n",
    "                \n",
    "                # Merge class 1 (people) into class 0 (pedestrian -> person)\n",
    "                cls[people_mask] = 0\n",
    "                \n",
    "                # Shift classes > 1 down by 1\n",
    "                gt1_mask = cls > 1\n",
    "                shifted_count += np.sum(gt1_mask)\n",
    "                cls[gt1_mask] -= 1\n",
    "                \n",
    "                # Store modified labels\n",
    "                labels[i]['cls'] = cls\n",
    "        \n",
    "        # Now set correct class count and names for training\n",
    "        if hasattr(self, 'data'):\n",
    "            # Update names and class count\n",
    "            self.data['names'] = self.merged_names\n",
    "            self.data['nc'] = len(self.merged_names)\n",
    "        \n",
    "        # Log statistics\n",
    "        person_count = sum(np.sum(label['cls'] == 0) for label in labels)\n",
    "        LOGGER.info(f\"\\n{colorstr('VisDroneDataset:')} Remapped {people_count} 'people' instances to {self.merged_names[0]}\")\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Total 'persona' instances after merge: {person_count}\")\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Shifted {shifted_count} instances of other classes\")\n",
    "        \n",
    "        return labels\n",
    "\n",
    "class MergedClassDetectionTrainer(DetectionTrainer):\n",
    "    \"\"\"\n",
    "    Custom trainer that uses VisDroneDataset for merged class training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_dataset(self, img_path, mode=\"train\", batch=None):\n",
    "        \"\"\"Build custom VisDroneDataset.\"\"\"\n",
    "        return VisDroneDataset(\n",
    "            img_path=img_path,\n",
    "            imgsz=self.args.imgsz,\n",
    "            batch_size=batch or self.batch_size,\n",
    "            augment=mode == \"train\",\n",
    "            hyp=self.args,\n",
    "            rect=self.args.rect if mode == \"train\" else True,\n",
    "            cache=self.args.cache or None,\n",
    "            single_cls=self.args.single_cls,\n",
    "            stride=self.stride,\n",
    "            pad=0.0 if mode == \"train\" else 0.5,\n",
    "            prefix=colorstr(f\"{mode}: \"),\n",
    "            task=self.args.task,\n",
    "            classes=None,\n",
    "            data=self.data,\n",
    "            fraction=self.args.fraction if mode == \"train\" else 1.0,\n",
    "        )\n",
    "    \n",
    "    def set_model_attributes(self):\n",
    "        \"\"\"Update model attributes for merged classes.\"\"\"\n",
    "        # First call parent method to set standard attributes\n",
    "        super().set_model_attributes()\n",
    "        \n",
    "        # Then update model with the merged class names\n",
    "        if hasattr(self.model, 'names'):\n",
    "            # Use the merged names directly from the dataset class\n",
    "            self.model.names = VisDroneDataset.merged_names\n",
    "            self.model.nc = len(VisDroneDataset.merged_names)\n",
    "            \n",
    "            # Also update data dictionary\n",
    "            if hasattr(self, 'data'):\n",
    "                self.data['names'] = VisDroneDataset.merged_names\n",
    "                self.data['nc'] = len(VisDroneDataset.merged_names)\n",
    "\n",
    "from ultralytics.models.yolo.detect import DetectionValidator\n",
    "\n",
    "class MergedClassDetectionValidator(DetectionValidator):\n",
    "    \"\"\"\n",
    "    Custom validator that uses VisDroneDataset for validation/testing with merged classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_dataset(self, img_path, mode=\"val\", batch=None):\n",
    "        \"\"\"Build custom VisDroneDataset for validation.\"\"\"\n",
    "        return VisDroneDataset(\n",
    "            img_path=img_path,\n",
    "            imgsz=self.args.imgsz,\n",
    "            batch_size=batch or self.args.batch,\n",
    "            augment=False,  # no augmentation during validation\n",
    "            hyp=self.args,\n",
    "            rect=True,  # rectangular validation for better performance\n",
    "            cache=None,\n",
    "            single_cls=self.args.single_cls,\n",
    "            stride=self.stride,\n",
    "            pad=0.5,\n",
    "            prefix=colorstr(f\"{mode}: \"),\n",
    "            task=self.args.task,\n",
    "            classes=self.args.classes,\n",
    "            data=self.data,\n",
    "        )\n",
    "    \n",
    "    def set_model_attributes(self):\n",
    "        \"\"\"Update model attributes for merged classes if using a PyTorch model.\"\"\"\n",
    "        super().set_model_attributes()\n",
    "        \n",
    "        # Update model names if it's a PyTorch model (not for exported models)\n",
    "        if hasattr(self.model, 'names') and hasattr(self.model, 'model'):\n",
    "            self.model.names = VisDroneDataset.merged_names\n",
    "            if hasattr(self.data, 'names'):\n",
    "                self.data['names'] = VisDroneDataset.merged_names\n",
    "                self.data['nc'] = len(VisDroneDataset.merged_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JdA0A5b3Eli2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Models by Fitness Score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>fitness</th>\n",
       "      <th>metrics/mAP50(B)</th>\n",
       "      <th>metrics/mAP50-95(B)</th>\n",
       "      <th>metrics/precision(B)</th>\n",
       "      <th>metrics/recall(B)</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr0</th>\n",
       "      <th>lrf</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>cos_lr</th>\n",
       "      <th>imgsz</th>\n",
       "      <th>box</th>\n",
       "      <th>cls</th>\n",
       "      <th>dfl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_tune_8290d_00015</td>\n",
       "      <td>0.184317</td>\n",
       "      <td>0.30057</td>\n",
       "      <td>0.17140</td>\n",
       "      <td>0.42157</td>\n",
       "      <td>0.30252</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.872225</td>\n",
       "      <td>0.947276</td>\n",
       "      <td>4.117353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_tune_9e260_00006</td>\n",
       "      <td>0.183925</td>\n",
       "      <td>0.29899</td>\n",
       "      <td>0.17114</td>\n",
       "      <td>0.39727</td>\n",
       "      <td>0.30826</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "      <td>640.0</td>\n",
       "      <td>5.120607</td>\n",
       "      <td>0.816281</td>\n",
       "      <td>4.608205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_tune_8290d_00008</td>\n",
       "      <td>0.183161</td>\n",
       "      <td>0.29657</td>\n",
       "      <td>0.17056</td>\n",
       "      <td>0.39720</td>\n",
       "      <td>0.30612</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.427384</td>\n",
       "      <td>0.759476</td>\n",
       "      <td>3.358324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_tune_8290d_00012</td>\n",
       "      <td>0.182955</td>\n",
       "      <td>0.29973</td>\n",
       "      <td>0.16998</td>\n",
       "      <td>0.42051</td>\n",
       "      <td>0.30218</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "      <td>640.0</td>\n",
       "      <td>7.015643</td>\n",
       "      <td>1.946285</td>\n",
       "      <td>4.618466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_tune_9e260_00015</td>\n",
       "      <td>0.182941</td>\n",
       "      <td>0.29761</td>\n",
       "      <td>0.17020</td>\n",
       "      <td>0.40803</td>\n",
       "      <td>0.30691</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "      <td>640.0</td>\n",
       "      <td>8.909576</td>\n",
       "      <td>0.940505</td>\n",
       "      <td>0.985378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name   fitness  metrics/mAP50(B)  metrics/mAP50-95(B)  \\\n",
       "0  _tune_8290d_00015  0.184317           0.30057              0.17140   \n",
       "1  _tune_9e260_00006  0.183925           0.29899              0.17114   \n",
       "2  _tune_8290d_00008  0.183161           0.29657              0.17056   \n",
       "3  _tune_8290d_00012  0.182955           0.29973              0.16998   \n",
       "4  _tune_9e260_00015  0.182941           0.29761              0.17020   \n",
       "\n",
       "   metrics/precision(B)  metrics/recall(B) optimizer    lr0   lrf  momentum  \\\n",
       "0               0.42157            0.30252     AdamW  0.001  0.10       0.8   \n",
       "1               0.39727            0.30826     AdamW  0.001  0.10       0.8   \n",
       "2               0.39720            0.30612     AdamW  0.001  0.01       0.8   \n",
       "3               0.42051            0.30218     AdamW  0.001  0.01       0.8   \n",
       "4               0.40803            0.30691     AdamW  0.001  0.10       0.8   \n",
       "\n",
       "   weight_decay cos_lr  imgsz       box       cls       dfl  \n",
       "0         0.001   True  640.0  3.872225  0.947276  4.117353  \n",
       "1         0.000   True  640.0  5.120607  0.816281  4.608205  \n",
       "2         0.000   True  640.0  3.427384  0.759476  3.358324  \n",
       "3         0.000   True  640.0  7.015643  1.946285  4.618466  \n",
       "4         0.000   True  640.0  8.909576  0.940505  0.985378  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load top 5 tune results by fitness\n",
    "\n",
    "csv_path = \"../data/processed/wandb_export_2025-03-05T10_24_46.923+01_00.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.rename(columns={'Name': 'name'})\n",
    "\n",
    "df['fitness'] = df['metrics/mAP50(B)'] * 0.1 + df['metrics/mAP50-95(B)'] * 0.9\n",
    "df_sorted = df.sort_values(by='fitness', ascending=False).head(5)\n",
    "\n",
    "columns_to_show = ['name', 'fitness', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)',\n",
    "                  'metrics/precision(B)', 'metrics/recall(B)', 'optimizer',\n",
    "                  'lr0', 'lrf', 'momentum', 'weight_decay', 'cos_lr', 'imgsz', 'box', 'cls', 'dfl']\n",
    "\n",
    "top_5 = df_sorted[columns_to_show].reset_index(drop=True)\n",
    "print(\"Top 5 Models by Fitness Score:\")\n",
    "display(top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache after each run\n",
    "\n",
    "def clear_cache():\n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Clear Python garbage collector\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "\n",
    "def save_results(dir, name, results):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_path = f\"{dir}/{name}_{timestamp}.json\"\n",
    "\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4, default=str)\n",
    "    \n",
    "    print(f\"{name} results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO('yolo12n.yaml')\n",
    "\n",
    "# # Train with the custom trainer\n",
    "# results = model.train(\n",
    "#     **config['train'],\n",
    "#     trainer=MergedClassDetectionTrainer\n",
    "#     )\n",
    "\n",
    "# test_results = model.val(\n",
    "#     **config['val'],\n",
    "#     validator=MergedClassValidator\n",
    "#     )\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(model: YOLO, config):\n",
    "    train_results = model.train(\n",
    "        **config['train'],\n",
    "        trainer=MergedClassDetectionTrainer\n",
    "        )\n",
    "\n",
    "    test_results = model.val(\n",
    "        **config['val'],\n",
    "        validator=MergedClassDetectionValidator\n",
    "        )\n",
    "\n",
    "    wandb.finish()\n",
    "    return train_results, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.85  Python-3.10.0 torch-2.5.1+cpu CPU (12th Gen Intel Core(TM) i7-1270P)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo12n, data=VisDrone.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=10, cache=False, device=None, workers=8, project=EyeInTheSky, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=True, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=True, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=10, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=EyeInTheSky\\train\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    432622  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "YOLO12n summary: 272 layers, 2,569,998 parameters, 2,569,982 gradients, 6.5 GFLOPs\n",
      "\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\labels.cache... 6471 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6471/6471 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\images\\0000137_02220_d_0000163.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\images\\0000140_00118_d_0000002.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\images\\9999945_00000_d_0000114.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\images\\9999987_00000_d_0000049.jpg: 1 duplicate labels removed\n",
      "\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Remapped 27059 'people' instances to persona\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Total 'persona' instances after merge: 106394\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Shifted 236807 instances of other classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Using merged classes: {0: 'persona', 1: 'bicicletta', 2: 'auto', 3: 'furgone', 4: 'camion', 5: 'triciclo', 6: 'triciclo-tendato', 7: 'autobus', 8: 'motociclo'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-val\\labels.cache... 548 images, 0 backgrounds, 0 corrupt: 100%|██████████| 548/548 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Remapped 5125 'people' instances to persona\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Total 'persona' instances after merge: 13969\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Shifted 24790 instances of other classes\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Using merged classes: {0: 'persona', 1: 'bicicletta', 2: 'auto', 3: 'furgone', 4: 'camion', 5: 'triciclo', 6: 'triciclo-tendato', 7: 'autobus', 8: 'motociclo'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to EyeInTheSky\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000769, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mEyeInTheSky\\train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      4.742      5.236      3.603        525        640: 100%|██████████| 405/405 [1:20:35<00:00, 11.94s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [01:19<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759      0.119    0.00885    0.00286     0.0008\n",
      "\n",
      "1 epochs completed in 1.366 hours.\n",
      "Optimizer stripped from EyeInTheSky\\train\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from EyeInTheSky\\train\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating EyeInTheSky\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.85  Python-3.10.0 torch-2.5.1+cpu CPU (12th Gen Intel Core(TM) i7-1270P)\n",
      "YOLO12n summary (fused): 159 layers, 2,558,678 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:47<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00837    0.00886    0.00281   0.000796\n",
      "               persona        531      13969          0          0          0          0\n",
      "            bicicletta        364       1287          0          0          0          0\n",
      "                  auto        515      14064     0.0583     0.0772     0.0206    0.00574\n",
      "               furgone        421       1975      0.017    0.00253    0.00368    0.00104\n",
      "                camion        266        750          0          0   0.000837    0.00036\n",
      "              triciclo        337       1045          0          0          0          0\n",
      "      triciclo-tendato        220        532          0          0          0          0\n",
      "               autobus        131        251          0          0   0.000152   3.04e-05\n",
      "             motociclo        485       4886          0          0          0          0\n",
      "Speed: 0.4ms preprocess, 57.1ms inference, 0.0ms loss, 15.2ms postprocess per image\n",
      "Results saved to \u001b[1mEyeInTheSky\\train\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▁</td></tr><tr><td>lr/pg1</td><td>▁</td></tr><tr><td>lr/pg2</td><td>▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▁</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁</td></tr><tr><td>metrics/precision(B)</td><td>▁</td></tr><tr><td>metrics/recall(B)</td><td>▁</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>▁</td></tr><tr><td>train/cls_loss</td><td>▁</td></tr><tr><td>train/dfl_loss</td><td>▁</td></tr><tr><td>val/box_loss</td><td>▁</td></tr><tr><td>val/cls_loss</td><td>▁</td></tr><tr><td>val/dfl_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>8e-05</td></tr><tr><td>lr/pg1</td><td>8e-05</td></tr><tr><td>lr/pg2</td><td>8e-05</td></tr><tr><td>metrics/mAP50(B)</td><td>0.00281</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.0008</td></tr><tr><td>metrics/precision(B)</td><td>0.00837</td></tr><tr><td>metrics/recall(B)</td><td>0.00886</td></tr><tr><td>model/GFLOPs</td><td>6.489</td></tr><tr><td>model/parameters</td><td>2569998</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>57.091</td></tr><tr><td>train/box_loss</td><td>4.74197</td></tr><tr><td>train/cls_loss</td><td>5.23559</td></tr><tr><td>train/dfl_loss</td><td>3.60298</td></tr><tr><td>val/box_loss</td><td>3.80521</td></tr><tr><td>val/cls_loss</td><td>3.75691</td></tr><tr><td>val/dfl_loss</td><td>2.4276</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolo12n_VisDrone.yaml_train</strong> at: <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged/runs/ao3amsey' target=\"_blank\">https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged/runs/ao3amsey</a><br> View project at: <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged' target=\"_blank\">https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged</a><br>Synced 5 W&B file(s), 29 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250308_164224-ao3amsey\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.85  Python-3.10.0 torch-2.5.1+cpu CPU (12th Gen Intel Core(TM) i7-1270P)\n",
      "YOLO12n summary (fused): 159 layers, 2,558,678 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-test-dev\\labels... 1610 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1610/1610 [00:04<00:00, 322.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-test-dev\\labels.cache\n",
      "\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Remapped 6376 'people' instances to persona\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Total 'persona' instances after merge: 27382\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Shifted 47720 instances of other classes\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Using merged classes: {0: 'persona', 1: 'bicicletta', 2: 'auto', 3: 'furgone', 4: 'camion', 5: 'triciclo', 6: 'triciclo-tendato', 7: 'autobus', 8: 'motociclo'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [34:31<00:00, 20.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1610      75102    0.00667    0.00133    0.00363    0.00101\n",
      "               persona       1267      27382          0          0          0          0\n",
      "            bicicletta        377       1302          0          0          0          0\n",
      "                  auto       1530      28074       0.06      0.012     0.0326    0.00905\n",
      "               furgone       1168       5771          0          0          0          0\n",
      "                camion        750       2659          0          0          0          0\n",
      "              triciclo        245        530          0          0          0          0\n",
      "      triciclo-tendato        233        599          0          0          0          0\n",
      "               autobus        838       2940          0          0          0          0\n",
      "             motociclo        794       5845          0          0          0          0\n",
      "Speed: 0.4ms preprocess, 1273.9ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mEyeInTheSky\\YOLOv12-VisDrone-Validation\u001b[0m\n",
      "train results saved to ../data/processed/train_20250308_184020.json\n",
      "test results saved to ../data/processed/test_20250308_184020.json\n",
      "Ultralytics 8.3.85  Python-3.10.0 torch-2.5.1+cpu CPU (12th Gen Intel Core(TM) i7-1270P)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo12n, data=VisDrone.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=10, cache=False, device=None, workers=8, project=EyeInTheSky, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=True, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=True, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=10, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=EyeInTheSky\\train2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    432622  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "YOLO12n summary: 272 layers, 2,569,998 parameters, 2,569,982 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 126/691 items from pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\franc\\Documents\\Repository\\DeepLearning-DroneRoadAccidentDetection\\notebooks\\wandb\\run-20250308_184021-dt7kc01l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky/runs/dt7kc01l' target=\"_blank\">train2</a></strong> to <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky' target=\"_blank\">https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky/runs/dt7kc01l' target=\"_blank\">https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky/runs/dt7kc01l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.21.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\labels.cache... 6471 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6471/6471 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\images\\0000137_02220_d_0000163.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\images\\0000140_00118_d_0000002.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\images\\9999945_00000_d_0000114.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-train\\images\\9999987_00000_d_0000049.jpg: 1 duplicate labels removed\n",
      "\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Remapped 27059 'people' instances to persona\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Total 'persona' instances after merge: 106394\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Shifted 236807 instances of other classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Using merged classes: {0: 'persona', 1: 'bicicletta', 2: 'auto', 3: 'furgone', 4: 'camion', 5: 'triciclo', 6: 'triciclo-tendato', 7: 'autobus', 8: 'motociclo'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-val\\labels.cache... 548 images, 0 backgrounds, 0 corrupt: 100%|██████████| 548/548 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Remapped 5125 'people' instances to persona\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Total 'persona' instances after merge: 13969\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Shifted 24790 instances of other classes\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Using merged classes: {0: 'persona', 1: 'bicicletta', 2: 'auto', 3: 'furgone', 4: 'camion', 5: 'triciclo', 6: 'triciclo-tendato', 7: 'autobus', 8: 'motociclo'}\n",
      "Plotting labels to EyeInTheSky\\train2\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000769, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mEyeInTheSky\\train2\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      3.826      3.832      2.527        525        640: 100%|██████████| 405/405 [1:07:58<00:00, 10.07s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [01:23<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759      0.566     0.0124    0.00902    0.00271\n",
      "\n",
      "1 epochs completed in 1.157 hours.\n",
      "Optimizer stripped from EyeInTheSky\\train2\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from EyeInTheSky\\train2\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating EyeInTheSky\\train2\\weights\\best.pt...\n",
      "Ultralytics 8.3.85  Python-3.10.0 torch-2.5.1+cpu CPU (12th Gen Intel Core(TM) i7-1270P)\n",
      "YOLO12n summary (fused): 159 layers, 2,558,678 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:50<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759      0.566     0.0124    0.00905    0.00273\n",
      "               persona        531      13969          1          0     0.0445     0.0139\n",
      "            bicicletta        364       1287          1          0          0          0\n",
      "                  auto        515      14064     0.0805      0.107     0.0299    0.00863\n",
      "               furgone        421       1975     0.0161    0.00506    0.00491    0.00138\n",
      "                camion        266        750          0          0    0.00167   0.000505\n",
      "              triciclo        337       1045          1          0          0          0\n",
      "      triciclo-tendato        220        532          1          0          0          0\n",
      "               autobus        131        251          0          0   0.000464   8.08e-05\n",
      "             motociclo        485       4886          1          0          0          0\n",
      "Speed: 0.4ms preprocess, 61.6ms inference, 0.0ms loss, 14.9ms postprocess per image\n",
      "Results saved to \u001b[1mEyeInTheSky\\train2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▁</td></tr><tr><td>lr/pg1</td><td>▁</td></tr><tr><td>lr/pg2</td><td>▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▁</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁</td></tr><tr><td>metrics/precision(B)</td><td>▁</td></tr><tr><td>metrics/recall(B)</td><td>▁</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>▁</td></tr><tr><td>train/cls_loss</td><td>▁</td></tr><tr><td>train/dfl_loss</td><td>▁</td></tr><tr><td>val/box_loss</td><td>▁</td></tr><tr><td>val/cls_loss</td><td>▁</td></tr><tr><td>val/dfl_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>8e-05</td></tr><tr><td>lr/pg1</td><td>8e-05</td></tr><tr><td>lr/pg2</td><td>8e-05</td></tr><tr><td>metrics/mAP50(B)</td><td>0.00905</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.00273</td></tr><tr><td>metrics/precision(B)</td><td>0.56629</td></tr><tr><td>metrics/recall(B)</td><td>0.01244</td></tr><tr><td>model/GFLOPs</td><td>6.489</td></tr><tr><td>model/parameters</td><td>2569998</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>61.578</td></tr><tr><td>train/box_loss</td><td>3.8257</td></tr><tr><td>train/cls_loss</td><td>3.83186</td></tr><tr><td>train/dfl_loss</td><td>2.52743</td></tr><tr><td>val/box_loss</td><td>3.63252</td></tr><tr><td>val/cls_loss</td><td>3.37449</td></tr><tr><td>val/dfl_loss</td><td>2.17182</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train2</strong> at: <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky/runs/dt7kc01l' target=\"_blank\">https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky/runs/dt7kc01l</a><br> View project at: <a href='https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky' target=\"_blank\">https://wandb.ai/francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky</a><br>Synced 5 W&B file(s), 30 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250308_184021-dt7kc01l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.85  Python-3.10.0 torch-2.5.1+cpu CPU (12th Gen Intel Core(TM) i7-1270P)\n",
      "YOLO12n summary (fused): 159 layers, 2,558,678 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\franc\\Documents\\Repository\\datasets\\VisDrone\\VisDrone2019-DET-test-dev\\labels.cache... 1610 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1610/1610 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Remapped 6376 'people' instances to persona\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Total 'persona' instances after merge: 27382\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Shifted 47720 instances of other classes\n",
      "\u001b[34m\u001b[1mVisDroneDataset:\u001b[0m Using merged classes: {0: 'persona', 1: 'bicicletta', 2: 'auto', 3: 'furgone', 4: 'camion', 5: 'triciclo', 6: 'triciclo-tendato', 7: 'autobus', 8: 'motociclo'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  71%|███████▏  | 72/101 [27:04<12:10, 25.17s/it]"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolo12n.yaml')\n",
    "df_train = top_5[['name', 'optimizer', 'lr0', 'lrf', 'momentum', 'weight_decay', 'cos_lr', 'box', 'cls', 'dfl']]\n",
    "\n",
    "for idx, trial in df_train.iterrows():\n",
    "    trial_config = config.copy()\n",
    "\n",
    "    trial_config.update(trial)\n",
    "\n",
    "    train_results, test_results = start(model, trial_config)\n",
    "\n",
    "    save_results(\"../data/processed\", \"train\", train_results)\n",
    "    save_results(\"../data/processed\", \"test\", test_results)\n",
    "\n",
    "    clear_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
